---
title: "Disney Plus"
author: "Yeni Hwang"
date: "11/24/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1) Data Loading and Cleaning

This is a dataset from kaggle <https://www.kaggle.com/unanimad/disney-plus-shows>.


```{r ,echo=FALSE, message = FALSE,warning=FALSE}

data<- read.csv("C:/Users/1/Desktop/StudayA/disney plus show/disney_plus_shows.csv")
library(dplyr)
##but leave rated's N/A as another factor
##turn award's N/A into 0
data <- subset(data, !title=="")

summary(data)



##Drop columns: year, metascore, added_at
##added_at can affect preference but over 700 data was skewed so drop them
##metasocre over 600 are na so drop
dropcol <- c("year","metascore","added_at")
data <- data[ , !names(data) %in% dropcol] 


#turn N/A into na value except rated, award
##but leave rated's N/A(152) as another factor

data[data=="N/A"] <-NA

data$rated = factor(data$rated, levels=c(levels(data$rated), "NA"))
data$rated[is.na(data$rated)] = "NA"

#type factor
levels(data$type)
data$type = factor(data$type, levels=c("episode","movie","series"))

#runtime eliminate " min"
library(stringr)
data$runtime <- str_replace(data$runtime, " min", "")
data$runtime <- factor(data$runtime)

#released at turn into months from latest film
data$released_at <- as.Date(data$released_at, format="%d %b %Y")
data$months_from_latest<- as.numeric(round((max(data$released_at, na.rm = TRUE) - data$released_at)/(365.25/12)))
dropcol <- c("released_at")
data <- data[ , !names(data) %in% dropcol] 

##extract numbers from "awards"
Nom<-as.numeric(str_extract(str_extract(data$awards, "for [[:digit:]]+"), "[[:digit:]]+"))
Nom2<-as.numeric(str_extract(str_extract(data$awards, "[[:digit:]]+ nom"), "[[:digit:]]+"))
Nom[is.na(Nom)] = 0
Nom2[is.na(Nom2)] =0
data$Nominates <-Nom+Nom2

wins_A <-as.numeric(str_extract(str_extract(data$awards, "Won [[:digit:]]+"), "[[:digit:]]+"))
wins_B <-as.numeric(str_extract(str_extract(data$awards, "[[:digit:]]+ win"), "[[:digit:]]+"))
wins_A[is.na(wins_A)] = 0
wins_B[is.na(wins_B)] = 0
data$wins<-wins_A+wins_B

summary(data)




####manually set "country" and "language" as a factor
#create dummy variables for "country", "language"
dataf <- data.frame(data$language)
colnames(dataf)[which(names(dataf) == colnames(dataf))] <- "language_E"
dataf$language_E <- ifelse(grepl("English",data$language),1,0)
dataf$language_M <- ifelse(grepl("Mandarin",data$language),1,0)
dataf$language_S <- ifelse(grepl("Spanish",data$language),1,0)
dataf$language_J <- ifelse(grepl("Japanese",data$language),1,0)
dataf$language_F <- ifelse(grepl("French",data$language),1,0)
dataf$language_AR <- ifelse(grepl("Arabic",data$language),1,0)
dataf$language_CH <- ifelse(grepl("Chinese",data$language),1,0)
dataf$language_KR <- ifelse(grepl("Korean",data$language),1,0)
dataf$language_AL <- ifelse(grepl("Algonquin",data$language),1,0)
dataf$language_BSL <- ifelse(grepl("Brazilian Sign Language",data$language),1,0)
dataf$language_ASL <- ifelse(grepl("American Sign Language",data$language),1,0)
dataf$language_CT <- ifelse(grepl("Cantonese",data$language),1,0)
dataf$language_GM <- ifelse(grepl("German",data$language),1,0)
dataf$language_T <- ifelse(grepl("Turkish",data$language),1,0)
dataf$language_H <- ifelse(grepl("Hindi",data$language),1,0)
dataf$language_D <- ifelse(grepl("Dutch",data$language),1,0)
dataf$language_CZ <- ifelse(grepl("Czech",data$language),1,0)
dataf$language_IT <- ifelse(grepl("Italian",data$language),1,0)
dataf$language_RS <- ifelse(grepl("Russian",data$language),1,0)
dataf$language_RO <- ifelse(grepl("Romanian",data$language),1,0)
dataf$language_X <- ifelse(grepl("Xhosa",data$language),1,0)
dataf$language_GR <- ifelse(grepl("Greek",data$language),1,0)
dataf$language_HW <- ifelse(grepl("Hawaiian",data$language),1,0)
dataf$language_IN <- ifelse(grepl("Indonesian",data$language),1,0)
dataf$language_BG <- ifelse(grepl("Bengali",data$language),1,0)
dataf$language_AK <- ifelse(grepl("Abkhazian",data$language),1,0)
dataf$language_KG <- ifelse(grepl("Klingon",data$language),1,0)
dataf$language_HB <- ifelse(grepl("Hebrew",data$language),1,0)
dataf$language_SW <- ifelse(grepl("Swedish",data$language),1,0)
dataf$language_SH <- ifelse(grepl("Swahili",data$language),1,0)
dataf$language_PO <- ifelse(grepl("Polish",data$language),1,0)
dataf$language_PT <- ifelse(grepl("Portuguese",data$language),1,0)
dataf$language_TH <- ifelse(grepl("Thai",data$language),1,0)
dataf$language_IK <- ifelse(grepl("Inuktitut",data$language),1,0)
dataf$language_CT <- ifelse(grepl("Croatian",data$language),1,0)
dataf$language_SB <- ifelse(grepl("Serbian",data$language),1,0)
dataf$language_TL <- ifelse(grepl("Telugu",data$language),1,0)
dataf$language_IR <- ifelse(grepl("Irish",data$language),1,0)
dataf$language_YD <- ifelse(grepl("Yiddish",data$language),1,0)
dataf$language_LT <- ifelse(grepl("Latin",data$language),1,0)
dataf$language_NW <- ifelse(grepl("Norwegian",data$language),1,0)
dataf$language_TB <- ifelse(grepl("Tibetan",data$language),1,0)
dataf$language_MG <- ifelse(grepl("Mongolian",data$language),1,0)
dataf$language_KZ <- ifelse(grepl("Kazakh",data$language),1,0)
dataf$language_NM <- ifelse(grepl("Nama",data$language),1,0)
dataf$language_HG <- ifelse(grepl("Hungarian",data$language),1,0)
dataf$language_ZL <- ifelse(grepl("Zulu",data$language),1,0)
dataf$language_UK <- ifelse(grepl("Ukrainian",data$language),1,0)
dataf$language_PR <- ifelse(grepl("Persian",data$language),1,0)
dataf$language_VT <- ifelse(grepl("Vietnamese",data$language),1,0)
dataf$language_UD <- ifelse(grepl("Urdu",data$language),1,0)



data$country_AR <- ifelse(grepl("Argentina",data$country),1,0)
data$country_MX <- ifelse(grepl("Mexico",data$country),1,0)
data$country_UK <- ifelse(grepl("UK",data$country),1,0)
data$country_US <- ifelse(grepl("USA",data$country),1,0)
data$country_CN <- ifelse(grepl("Canada",data$country),1,0)
data$country_ML <- ifelse(grepl("Malaysia",data$country),1,0)
data$country_FR <- ifelse(grepl("France",data$country),1,0)
data$country_CH <- ifelse(grepl("China",data$country),1,0)
data$country_HK <- ifelse(grepl("Hong Kong",data$country),1,0)
data$country_DM <- ifelse(grepl("Denmark",data$country),1,0)
data$country_GM <- ifelse(grepl("Germany",data$country),1,0)
data$country_KZ <- ifelse(grepl("Kazahstan",data$country),1,0)
data$country_AS <- ifelse(grepl("Austrailia",data$country),1,0)
data$country_IN <- ifelse(grepl("India",data$country),1,0)
data$country_IR <- ifelse(grepl("Ireland",data$country),1,0)
data$country_SK <- ifelse(grepl("South Korea",data$country),1,0)
data$country_SP <- ifelse(grepl("Spain",data$country),1,0)
data$country_PL <- ifelse(grepl("Poland",data$country),1,0)
data$country_HG <- ifelse(grepl("Hungary",data$country),1,0)
data$country_NZ <- ifelse(grepl("New Zealand",data$country),1,0)
data$country_NW <- ifelse(grepl("Norway",data$country),1,0)
data$country_SY <- ifelse(grepl("Syria",data$country),1,0)
data$country_TW <- ifelse(grepl("Taiwan",data$country),1,0)
data$country_SA <- ifelse(grepl("South Africa",data$country),1,0)
data$country_JP <- ifelse(grepl("Japan",data$country),1,0)
data$country_TZ <- ifelse(grepl("Tanzania",data$country),1,0)
data$country_AG <- ifelse(grepl("Angola",data$country),1,0)
data$country_BS <- ifelse(grepl("Botswana",data$country),1,0)
data$country_NM <- ifelse(grepl("Namibia",data$country),1,0)
data$country_PN <- ifelse(grepl("Panama",data$country),1,0)
data$country_SW <- ifelse(grepl("Sweden",data$country),1,0)
data$country_IN <- ifelse(grepl("Iran",data$country),1,0)
data$country_RS <- ifelse(grepl("Russia",data$country),1,0)
data$country_ML <- ifelse(grepl("Malaysia",data$country),1,0)
data$country_PH <- ifelse(grepl("Philippines",data$country),1,0)
data$country_SV <- ifelse(grepl("Slovenia",data$country),1,0)
data$country_CR <- ifelse(grepl("Czech Republic",data$country),1,0)
data$country_SP <- ifelse(grepl("Singaopre",data$country),1,0)
data$country_PK <- ifelse(grepl("Pakistan",data$country),1,0)
data$country_EG <- ifelse(grepl("Egypt",data$country),1,0)


```

## 2) EDA: Text Mining and wordcloud(bi-grams)

Wordcloud from current data of title, plot, genre:

```{r echo=FALSE, message=FALSE, warning=FALSE}

data$film <- paste(data$title, data$plot, data$genre )

##remove part of string like "(story)", "(teleplay)" 
data$film <-gsub("\\s*\\([^\\)]+\\)","",data$film)

####################################################
###1. EDA text data
##generate bigrams from data and show wordcloud
library(tidyverse)
library(tidytext)
library(dplyr)

# create a vector of all bi-grams from "title" and "plot" to keep 
data$film2 <- paste(data$title, data$plot)
ap.corpus <-VCorpus(VectorSource(data$film2))
####
#### work only with lower-case words
ap.corpus <- tm_map(ap.corpus, content_transformer(tolower))
#### remove punctuation
ap.corpus <- tm_map(ap.corpus, removePunctuation)
#### remove unhelpful words
ap.corpus <- tm_map(ap.corpus, removeWords, stopwords("english"))
#### stemming (remove variations from words)
ap.corpus <- tm_map(ap.corpus, stemDocument)
ap.corpus[[1]]$content

library(tm)
NLPbigramTokenizer <- function(x) {
  unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}

tdm_NLP <- TermDocumentMatrix(ap.corpus, control=list(tokenize = NLPbigramTokenizer))

tdm89.ng <- removeSparseTerms(tdm_NLP, 0.89)
tdm9.ng  <- removeSparseTerms(tdm_NLP, 0.9)
tdm91.ng <- removeSparseTerms(tdm_NLP, 0.91)
tdm92.ng <- removeSparseTerms(tdm_NLP, 0.92)

notsparse <- tdm89.ng
m = as.matrix(tdm_NLP)
v = sort(rowSums(m),decreasing=TRUE)
d = data.frame(word = names(v),freq=v)

library(wordcloud)
# Create the word cloud
wordcloud(words = d$word, freq = d$freq, min.freq = 0.008, random.order = F, col="navy")
```

## 3) Current limitation(kmeans) and Topic modeling

To dealt with Text data, I vectorized above selected text data (title, plot, genre) and tried k means clustering which results in showing limitation of conventional categorization
: 3 big cluster exists which means model cannot classify them well in depth.

Beneath the surface, to get latent topics from the data I used Topic modeling (LDA)
: automatically choose optimal number of topics, five and plot wordclouds for each topic.


```{r echo=FALSE, message=FALSE, warning=FALSE}


library(slam)
library(tm)
library(textir) # to get the data
library(RColorBrewer)
library(wordcloud)
library(lattice)
library(rjson)
library(maptpx) # for the topics function

####################################################
#####################################################
#### Preprocessing and Vectorized Text Data


ap.corpus <-VCorpus(VectorSource(data$film))
####
#### work only with lower-case words
ap.corpus <- tm_map(ap.corpus, content_transformer(tolower))
#### remove punctuation
ap.corpus <- tm_map(ap.corpus, removePunctuation)
#### remove unhelpful words
ap.corpus <- tm_map(ap.corpus, removeWords, stopwords("english"))
#### stemming (remove variations from words)
ap.corpus <- tm_map(ap.corpus, stemDocument)
ap.corpus[[1]]$content

#### Now we create a matrix 
#### where each document is a row
#### and each column corresponds to 
#### to the number of times a word appears on each document

ap.tdm <- DocumentTermMatrix(ap.corpus)
inspect(ap.tdm)
str(ap.tdm)
ap.m <- as.matrix(ap.tdm)
str(ap.m)

ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
### Total number of words after cleaning up
sum(ap.d$freq)



####################################################
#####################################################
#### Genre K Means
## present current limitation of genre category

# takes time, because you're making it dense from sparse
fs <- scale(as.matrix( ap.m/rowSums(ap.m) ))
## both the kmeans and topic modelling take a long time...
kmfs <- kmeans(fs,10)  
## you're fitting massively high dimensional models (K*ncol(x))
## there are approximate distributed algorithms out there
## for really big data, I think you just focus on subsamples, most common words, etc.
## all these methods find are the dominant sources of variation, 
## so those should be present in small subsamples

## interpretation: we can see the words with cluster centers
kmfs$centers

## highest above zero (these are in units of standard deviation of f)
print(apply(kmfs$centers,1,function(c) colnames(fs)[order(-c)[1:10]]))
## shows words that occur far more in these clusters than on average


#using 
kmfs$size
## we can see the cluster sizes and there is a big cluster
## [1]   4   1   1   1  25 858   1   1   1   1


####################################################
####################################################
### Topic Modeling
## topic modeling. Treat counts as actual counts!
## i.e., model them with a multinomial
## we'll use the topics function in maptpx (there are other options out there)

## you need to convert from a Matrix to a `slam' simple_triplet_matrix
x <- as.simple_triplet_matrix(ap.m)


## choosing the number of topics
## If you supply a vector of topic sizes, it uses a Bayes factor to choose
## (BF is like exp(-BIC), so you choose the bigggest BF) 
## the method stops if BF drops twice in a row
tpcs <- topics(x,K=seq(from=5,to=15,by=5)) # it chooses 5 topics 

## interpretation
## summary prints the top `n' words for each topic,
## under ordering by `topic over aggregate' lift:
##    the topic word prob over marginal word prob.
summary(tpcs, n=10) 
# this will promote rare words that with high in-topic prob

# alternatively, you can look at words ordered by simple in-topic prob
## the topic-term probability matrix is called 'theta', 
## and each column is a topic
## we can use these to rank terms by probability within topics
rownames(tpcs$theta)[order(tpcs$theta[,1], decreasing=TRUE)[1:10]]
rownames(tpcs$theta)[order(tpcs$theta[,2], decreasing=TRUE)[1:10]]
rownames(tpcs$theta)[order(tpcs$theta[,3], decreasing=TRUE)[1:10]]
rownames(tpcs$theta)[order(tpcs$theta[,4], decreasing=TRUE)[1:10]]
rownames(tpcs$theta)[order(tpcs$theta[,5], decreasing=TRUE)[1:10]]


tpcs$K
tpcs$theta##in topic prob of a word -> can show wordcloud for each topic
tpcs$omega###894 contents proportion of 5 topics
sum(tpcs$omega[,1])
sum(tpcs$omega[,2])
sum(tpcs$omega[,3])
sum(tpcs$omega[,4])
sum(tpcs$omega[,5])



library(wordcloud)
## we'll size the word proportional to its in-topic probability
## and only show those with > 0.004 omega
## (it will still likely warn that it couldn't fit everything)
par(mar=c(0,0,0,0))

wordcloud(row.names(tpcs$theta), 
          freq=tpcs$theta[,1], min.freq=0.002, random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Blues"))(32)))
wordcloud(row.names(tpcs$theta), 
          freq=tpcs$theta[,2], min.freq=0.002, random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Reds"))(32)))
wordcloud(row.names(tpcs$theta), 
          freq=tpcs$theta[,3], min.freq=0.002,random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Purples"))(32)))
wordcloud(row.names(tpcs$theta), 
          freq=tpcs$theta[,4], min.freq=0.002,random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Greens"))(32)))
wordcloud(row.names(tpcs$theta), 
          freq=tpcs$theta[,5], min.freq=0.002,random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Greys"))(32)))

```


## 4) K means clustering - topics and all other data

K means clustering with generated topics data and other features of shows preprocessed at the first stage.
It is interesting that "10 Things I Hate About You" and "Stargirl" are reclassified into different clusters even their genre was illustrated as same conventionally. We could find the reason regarding topics by plotting topic proprotion. 

```{r message=FALSE, warning=FALSE}


topicdata <- as.data.frame(tpcs$omega)

### Data setting (put features all together)
select <- c("type","rated","runtime","imdb_rating","imdb_votes","months_from_latest","Nominate","wins")
newdata <-data[,(names(data) %in% select)]
newdata <- sapply(newdata, unclass)
# multiple variables to numeric # by using unclass funciton
# normalization
newdata<-scale(newdata)

### Consolidate data
data_new <- cbind(newdata,dataf,topicdata)
summary(data_new)


### Computing the optimal number of clusters in our example:
kfit <- lapply(1:30, function(k) kmeans(na.omit(data_new),k,nstart=10))
# choose number of clusters based on the fit above
# we will use the  script kIC in DataAnalyticsFunctions.R
# We call the function kIC the performance of the various 
# kmeans for k=1,...50, that was stored in kfit.
# Then "A" for AICc (default) or "B" for BIC
kIC <- function(fit, rule=c("A","B","C")){
  df <- length(fit$centers) # K*dim
  n <- sum(fit$size)
  D <- fit$tot.withinss # deviance
  rule=match.arg(rule)
  if(rule=="A")
    #return(D + 2*df*n/max(1,n-df-1))
    return(D + 2*df)
  else if(rule=="B") 
    return(D + log(n)*df)
  else 
    return(D +  sqrt( n * log(df) )*df)
}

kaic <- sapply(kfit,kIC)
kbic  <- sapply(kfit,kIC,"B")
kHDic  <- sapply(kfit,kIC,"C")
which.min(kaic)
which.min(kbic)
which.min(kHDic)
## Now we plot them, first we plot AIC
par(mar=c(1,1,1,1))
par(mai=c(1,1,1,1))
plot(kaic, xlab="k (# of clusters)", ylab="IC (Deviance + Penalty)", 
     ylim=range(c(kaic,kbic,kHDic)), # get them on same page
     type="l", lwd=2)
# Vertical line where AIC is minimized
abline(v=which.min(kaic))
# Next we plot BIC
lines(kbic, col=4, lwd=2)
# Vertical line where BIC is minimized
abline(v=which.min(kbic),col=4)
# Next we plot HDIC
lines(kHDic, col=3, lwd=2)
# Vertical line where HDIC is minimized
abline(v=which.min(kHDic),col=3)

# Insert labels
text(c(which.min(kaic),which.min(kbic),which.min(kHDic)),c(mean(kaic),mean(kbic),mean(kHDic)),c("AIC","BIC","HDIC"))
# both AICc and BIC choose more complicated models


### Recomputing kmeans
Smultiple_kmeans <- kmeans(na.omit(data_new), 5, nstart=10)
Smultiple_kmeans$centers
Smultiple_kmeans$size


### Consolidate data and results of clustering to analyize
data_new$title <- data$title
data_new$plot <- data$plot
data_new$genre <- data$genre
summary(data_new)
data_new$plot = factor(data_new$plot, levels=c(levels(data_new$plot), "NA"))
data_new$plot[is.na(data_new$plot)] = "NA"
data_new$genre = factor(data_new$genre, levels=c(levels(data_new$genre), "NA"))
data_new$genre[is.na(data_new$genre)] = "NA"
data_new <- na.omit(data_new)

data_new$cluster <- factor(Smultiple_kmeans$cluster)

### To find the shows in same genre set.
data_new[data_new$genre == "Comedy, Drama, Romance",c("title","genre","cluster")]
d2<- data_new[data_new$genre == "Comedy, Drama, Romance",c("1","2","3","4","5")]


d2 <- data.frame(t(d2))
d2$`10 things`
colnames(d2) <- c("10 things", "Stargirl")

barplot(t(as.matrix(d2)), beside=TRUE, legend=TRUE, args.legend=c(xjust=1, yjust=0.5),
        ylab= bquote( "Proportion"), xlab="Topics",col=c("black","grey"), names.arg = c(1:5))


### Cluster (reclassified) distributions within two topics (e.g. topic 1 and 2)

library(ggplot2)

data_new$topic1 <- data_new$"1"
data_new$topic2 <- data_new$"2"
ggplot(data_new) + geom_point(mapping = aes(x = topic1, y = topic2, color = cluster))

```

## 5) Hierarchical clustering : Disney Plus

To quantify diversity of contents within disney plus, run Hierarchical clustering and get the quantifuied dissimilarties presented as Heights score in dendogram.

```{r message=FALSE, warning=FALSE}

####################################################
#############Hierarchical clustering (Disney plus)
####################################################

#### Note: you do not pass the data for hclust
#### you pass a distance matrix where each entry 
#### has the distance between 2 observations
#### that is we create a distance matrix with the data
DistanceMatrix <- dist(as.matrix(topicdata))
#### then we use it in the function hclust (not the data)
hcsimple <- hclust(DistanceMatrix)
plot(hcsimple)

memb <- cutree(hcsimple, k = 5)
table(memb)

## heights in ther first order is the dissimilarity of two last clusters
dx <- order(hcsimple$height, decreasing = T)[1:5]
hcsimple$height[dx]




```

## 6) Comparison with Hulu and Disney

To compare Disney's and Hulu (Only series/show):
As same with above, preprocess data (Only series/show) and run Topic modeling and Hierarchical clustering.


```{r message=FALSE, warning=FALSE}

hulu<- read.csv("C:/Users/1/Desktop/StudayA/disney plus show/hulu/HuluRaw.csv")
summary(hulu)
hulu$film <- paste(hulu$show.name, hulu$show.description, hulu$show.genre )
##remove part of string like "(story)", "(teleplay)" 
hulu$film <-gsub("\\s*\\([^\\)]+\\)","",hulu$film)

hulufilm <- hulu$film
hulufilm <- unique(hulufilm)


### To generate topic model
ap.corpus <-VCorpus(VectorSource(hulufilm))
ap.corpus[[1]]$content
ap.tdm <- DocumentTermMatrix(ap.corpus)
inspect(ap.tdm)

####
#### work only with lower-case words
ap.corpus <- tm_map(ap.corpus, content_transformer(tolower))
#### remove punctuation
ap.corpus <- tm_map(ap.corpus, removePunctuation)
#### remove unhelpful words
ap.corpus <- tm_map(ap.corpus, removeWords, stopwords("english"))
#### stemming (remove variations from words)
ap.corpus <- tm_map(ap.corpus, stemDocument)
ap.corpus[[1]]$content

ap.tdm <- DocumentTermMatrix(ap.corpus)
inspect(ap.tdm)
ap.m2 <- as.matrix(ap.tdm)
str(ap.m2)

ap.v2 <- sort(rowSums(ap.m2),decreasing=TRUE)
ap.d2 <- data.frame(word = names(ap.v2),freq=ap.v2)
### Total number of words after cleaning up
sum(ap.d2$freq)

####################################################
####################################################
### Hulu (series/show)

hulu_x <- as.simple_triplet_matrix(ap.m2)

## choosing the number of topics
## If you supply a vector of topic sizes, it uses a Bayes factor to choose
## (BF is like exp(-BIC), so you choose the bigggest BF) 
## the method stops if BF drops twice in a row
hulu_tpcs <- topics(hulu_x,K=seq(from=5,to=15,by=5)) # it chooses 5 topics 

## interpretation
## summary prints the top `n' words for each topic,
## under ordering by `topic over aggregate' lift:
##    the topic word prob over marginal word prob.
summary(hulu_tpcs, n=10) 
## we can use these to rank terms by probability within topics
rownames(hulu_tpcs$theta)[order(hulu_tpcs$theta[,1], decreasing=TRUE)[1:10]]
rownames(hulu_tpcs$theta)[order(hulu_tpcs$theta[,2], decreasing=TRUE)[1:10]]
rownames(hulu_tpcs$theta)[order(hulu_tpcs$theta[,3], decreasing=TRUE)[1:10]]
rownames(hulu_tpcs$theta)[order(hulu_tpcs$theta[,4], decreasing=TRUE)[1:10]]
rownames(hulu_tpcs$theta)[order(hulu_tpcs$theta[,5], decreasing=TRUE)[1:10]]


hulu_tpcs$K
hulu_tpcs$theta##in topic prob of a word -> can show wordcloud for each topic
hulu_tpcs$omega###109 contents proportion of 5 topics

### To not run and generate wordcloud "CTRL + SHIFT + C" below while selecting the chunck
par(mar=c(0,0,0,0))
wordcloud(row.names(hulu_tpcs$theta), 
          freq=hulu_tpcs$theta[,1], min.freq=0.002, random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Greens"))(32)))
wordcloud(row.names(hulu_tpcs$theta), 
          freq=hulu_tpcs$theta[,2], min.freq=0.002, random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Greens"))(32)))
wordcloud(row.names(hulu_tpcs$theta), 
          freq=hulu_tpcs$theta[,3], min.freq=0.002,random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Greens"))(32)))
wordcloud(row.names(hulu_tpcs$theta), 
          freq=hulu_tpcs$theta[,4], min.freq=0.002,random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Greens"))(32)))
wordcloud(row.names(hulu_tpcs$theta), 
          freq=hulu_tpcs$theta[,5], min.freq=0.002,random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Greens"))(32)))

hulu_topic <- as.data.frame(hulu_tpcs$omega)
DistanceMatrix <- dist(as.matrix(hulu_topic))
#### then we use it in the function hclust (not the data)
hcsimple_hulu <- hclust(DistanceMatrix)
par(mar=c(1,5,1,1))

plot(hcsimple_hulu)

### assume we're clustering them into 5 groups
dx <- order(hcsimple_hulu$height, decreasing = T)[1:5]

memb <- cutree(hcsimple_hulu, k = 5)
table(memb)

heights_hulu <- hcsimple_hulu$height[dx]


####################################################
####################################################
### Disney plus (series/show)

disneyshow <- data[data$type=="series",]
select <- c("title","plot","genre")
disneyshow <-disneyshow[,(names(disneyshow) %in% select)]


library(tidyr)
disneyshow <-disneyshow %>% drop_na(title,plot)
##randomly extract 109 data
#disneyshow <- disneyshow[sample(nrow(disneyshow), 109), ]
summary(disneyshow)
disneyshow <- paste(disneyshow$title, disneyshow$plot, disneyshow$genre)

##To generate topic model
ap.corpus <-VCorpus(VectorSource(disneyshow))
ap.tdm <- DocumentTermMatrix(ap.corpus)
inspect(ap.tdm)

####
#### work only with lower-case words
ap.corpus <- tm_map(ap.corpus, content_transformer(tolower))
#### remove punctuation
ap.corpus <- tm_map(ap.corpus, removePunctuation)
#### remove unhelpful words
ap.corpus <- tm_map(ap.corpus, removeWords, stopwords("english"))
#### stemming (remove variations from words)
ap.corpus <- tm_map(ap.corpus, stemDocument)
ap.corpus[[1]]$content


ap.tdm <- DocumentTermMatrix(ap.corpus)
inspect(ap.tdm)
str(ap.tdm)
ap.m3 <- as.matrix(ap.tdm)
str(ap.m3)


ap.v3 <- sort(rowSums(ap.m3),decreasing=TRUE)
ap.d3 <- data.frame(word = names(ap.v3),freq=ap.v3)
### Total number of words after cleaning up
sum(ap.d3$freq)

disney_x <- as.simple_triplet_matrix(ap.m3)


## choosing the number of topics
## If you supply a vector of topic sizes, it uses a Bayes factor to choose
## (BF is like exp(-BIC), so you choose the bigggest BF) 
## the method stops if BF drops twice in a row
disney_tpcs <- topics(disney_x,K=seq(from=5,to=15,by=5)) # it chooses 5 topics 

## interpretation
## summary prints the top `n' words for each topic,
## under ordering by `topic over aggregate' lift:
##    the topic word prob over marginal word prob.
summary(disney_tpcs, n=10) 
## we can use these to rank terms by probability within topics
rownames(disney_tpcs$theta)[order(disney_tpcs$theta[,1], decreasing=TRUE)[1:10]]
rownames(disney_tpcs$theta)[order(disney_tpcs$theta[,2], decreasing=TRUE)[1:10]]
rownames(disney_tpcs$theta)[order(disney_tpcs$theta[,3], decreasing=TRUE)[1:10]]
rownames(disney_tpcs$theta)[order(disney_tpcs$theta[,4], decreasing=TRUE)[1:10]]
rownames(disney_tpcs$theta)[order(disney_tpcs$theta[,5], decreasing=TRUE)[1:10]]


disney_tpcs$K
disney_tpcs$theta##in topic prob of a word -> can show wordcloud for each topic
disney_tpcs$omega###109 contents proportion of 5 topics

### To not generate wordcloud "CTRL + SHIFT + C" below while selecting the chunck
par(mar=c(1,1,1,1))
wordcloud(row.names(disney_tpcs$theta),
          freq=disney_tpcs$theta[,1], min.freq=0.002, random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Blues"))(32)))
wordcloud(row.names(disney_tpcs$theta),
          freq=disney_tpcs$theta[,2], min.freq=0.002, random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Blues"))(32)))
wordcloud(row.names(disney_tpcs$theta),
          freq=disney_tpcs$theta[,3], min.freq=0.002,random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Blues"))(32)))
wordcloud(row.names(disney_tpcs$theta),
          freq=disney_tpcs$theta[,4], min.freq=0.002,random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Blues"))(32)))
wordcloud(row.names(disney_tpcs$theta),
          freq=disney_tpcs$theta[,5], min.freq=0.002,random.order=FALSE, col=rev(colorRampPalette(brewer.pal(9,"Blues"))(32)))

disney_topic <- as.data.frame(disney_tpcs$omega)
DistanceMatrix <- dist(as.matrix(disney_topic))
#### then we use it in the function hclust (not the data)
hcsimple_dn <- hclust(DistanceMatrix)
par(mar=c(1,5,1,1))
plot(hcsimple_dn)
### assume we're clustering them into 5 groups
dx <- order(hcsimple_dn$height, decreasing = T)[1:5]

heights_disney <- hcsimple_dn$height[dx]


####################################################
####################################################
### Comparing Result of Hulu and Disney

heights_hulu##heights in ther first order is the maximum dissimilarity
heights_disney

library(knitr)
dt <- data.frame(heights_disney,heights_hulu, row.names = c("2 clusters","3 clusters","4 clusters","5 clusters","6 clusters"))
kable(dt, format = "html", caption ="Distances within clusters")

```


